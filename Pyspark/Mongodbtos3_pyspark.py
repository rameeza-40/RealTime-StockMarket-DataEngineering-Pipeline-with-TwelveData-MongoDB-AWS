# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1huRs5OQ7RUZgDuCTQFi3pbCjPvx1XQTA
"""

from gettext import install


pip install bson # type: ignore

pip install pymongo # type: ignore

pip install pymongo pandas # type: ignore

pip install boto3 # type: ignore


import bson
from pymongo import MongoClient
import pandas as pd
import boto3
from datetime import datetime
from bson.decimal128 import Decimal128
import os

#Step 1: Connect to MongoDB
client = MongoClient("mongodb+srv://Rameeza:rame@cluster0.mfh68.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0")
db = client["stock_db"]
collection = db["Ingestion_Dag"]

#Step 2: Fetch data from MongoDB
documents = list(collection.find())

#Step 3: Remove MongoDBâ€™s internal _id field
for doc in documents:
        doc.pop('_id', None)

#Step 4: Convert to Pandas DataFrame
pdf = pd.DataFrame(documents)

#Step 5: Convert Decimal128 to Decimal
for key in ["open", "high", "low", "close", "volume"]:
    if key in pdf.columns:
        pdf[key] = pdf[key].apply(lambda x: x.to_decimal() if isinstance(x, Decimal128) else x)

#Step 6: Convert 'datetime' to datetime type and 'symbol' to string
if 'datetime' in pdf.columns:
        pdf['datetime'] = pd.to_datetime(pdf['datetime'], errors='coerce')

if 'symbol' in pdf.columns:
        pdf['symbol'] = pdf['symbol'].astype(str)

print("Pandas DataFrame Info:")
pdf.info()
print("\nData Preview:")
print(pdf.head())

#Step 7: Define local file path (no timestamp)
local_file_path = "/tmp/stock_data.csv"

#Step 8: Save DataFrame to CSV
pdf.to_csv(local_file_path, index=False)
print(f"Data saved locally to {local_file_path}")

#Step 9: Configure boto3
aws_cred = {
"access_key": os.getenv("AWS_ACCESS_KEY_ID"), # Replace with your actual access key
"session_key": os.getenv("AWS_SECRET_ACCESS_KEY"), # Replace with your actual secret key # type: ignore
"region_name": "ap-south-1"
}

s3_client = boto3.client(
"s3",
aws_access_key_id=aws_cred["access_key"],
aws_secret_access_key=aws_cred["session_key"],
region_name=aws_cred["region_name"]
)

#Step 10: Define S3 bucket and key (no timestamp)
bucket_name = "guviproject" # Replace with your actual bucket name
s3_object_key = "stock_data.csv"

#Step 11: Upload to S3
try:
    s3_client.upload_file(local_file_path, bucket_name, s3_object_key)
    print(f"Uploaded to s3://{bucket_name}/{s3_object_key}")
except Exception as e:
    print(f"Upload error: {e}")

#Step 12: Clean up local file
if os.path.exists(local_file_path):
    os.remove(local_file_path)
    print(f"Removed local file {local_file_path}")
